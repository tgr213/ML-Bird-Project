{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c78db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04560cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '100-bird-species/train/'\n",
    "valid_dir = '100-bird-species/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bc4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tranforms.compose  == several operators\n",
    "## transorms.resize   == resize the image\n",
    "    ## Image.LANCZOS  == highest quality filter for resize\n",
    "## tranforms.ToTensor == converts image file to tensor array\n",
    "\n",
    "transformation = transforms.Compose([transforms.Resize((224, 224), Image.LANCZOS), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b963f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageFolder(train_dir, transform = transformation)\n",
    "valid_data = ImageFolder(valid_dir, transform = transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e71e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data loader returns one sample at a time, it reshuffles the data to reduce overfitting\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a2208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c13a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePatcher(nn.Module):\n",
    "    def __init__(self, image_size = 224, patch_size = 14, embedding_dim = 128):\n",
    "        super(ImagePatcher, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        ##defining function that will patch the image up\n",
    "        self.unfold = nn.Unfold(kernel_size = self.patch_size, stride = self.patch_size)\n",
    "        \n",
    "        ##defining the function that will do the linear embedding\n",
    "        self.linear = nn.Linear(self.patch_size*self.patch_size*3, self.embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #batch_size, c, h, w = x.shape\n",
    "        \n",
    "        ##patch the images\n",
    "        patches = self.unfold(x).permute(0, 2, 1) # [batch size, c*h*w, num_of_patches]\n",
    "        \n",
    "        ##make linear embedding\n",
    "        patches = self.linear(patches)\n",
    "        \n",
    "        return patches ## output should have dimensions [batch size, num_patches, embedding dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b304c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, embedding_dim = 128, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        ##normalizations\n",
    "        self.layer_norm_1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        ##self-attention block\n",
    "        self.attn = nn.MultiheadAttention(embedding_dim, num_heads, dropout, batch_first=True)\n",
    "        ##who the fuck thought batch_first = False should be the default?!?!?!?!!??!?!?!?!\n",
    "        ##bruhhh u stupid asf\n",
    "        \n",
    "        ##feed-forward function\n",
    "        ##  relu activation function, the linear functions just make you go back and forth\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=embedding_dim),\n",
    "            nn.Dropout(dropout))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #normalizing the tensor\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        \n",
    "        #residual connection\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        \n",
    "        #feed-forward\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        \n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07ac09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, num_layers, \n",
    "                 num_classes = 525, image_size = 224, embedding_dim = 128, patch_size = 14, dropout = 0.0):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size//patch_size)**2\n",
    "        \n",
    "        ##calling the previously defined functions:\n",
    "        self.patcher = ImagePatcher(image_size, patch_size, embedding_dim)\n",
    "        self.attentions = nn.ModuleList([Attention(hidden_dim=hidden_dim, num_heads=num_heads, \n",
    "                                                   embedding_dim=embedding_dim, dropout=dropout) \n",
    "                                         for _ in range(num_layers)])\n",
    "        \n",
    "        ##droput\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        ##do the classification thingy\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(normalized_shape=embedding_dim), \n",
    "                                      nn.Linear(embedding_dim, num_classes))\n",
    "        \n",
    "        ##classification token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embedding_dim))\n",
    "        \n",
    "        ##positional embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #data pre-processing\n",
    "        x = self.patcher(x)\n",
    "        batchSize, T, _ = x.shape\n",
    "        \n",
    "        #add classification token\n",
    "        cls_token = self.cls_token.repeat(batchSize, 1, 1)\n",
    "        x = torch.cat((cls_token, x), dim = 1)\n",
    "        x = self.pos_embedding[:,:T+1] + x\n",
    "        \n",
    "        #apply transformer\n",
    "        for trans in self.attentions:\n",
    "            x = self.dropout(x)\n",
    "            x = trans(x)\n",
    "        \n",
    "        #classification prediction\n",
    "        cls = x[:,0]\n",
    "        out = self.mlp_head(cls)\n",
    "        \n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79b78dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [1, 525]                  33,024\n",
       "├─ImagePatcher: 1-1                      [1, 256, 128]             --\n",
       "│    └─Unfold: 2-1                       [1, 588, 256]             --\n",
       "│    └─Linear: 2-2                       [1, 256, 128]             75,392\n",
       "├─Dropout: 1-2                           [1, 257, 128]             --\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Attention: 2-3                    [1, 257, 128]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 257, 128]             256\n",
       "│    │    └─MultiheadAttention: 3-2      [1, 257, 128]             66,048\n",
       "│    │    └─LayerNorm: 3-3               [1, 257, 128]             256\n",
       "│    │    └─Sequential: 3-4              [1, 257, 128]             131,712\n",
       "├─Dropout: 1-4                           [1, 257, 128]             --\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Attention: 2-4                    [1, 257, 128]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 257, 128]             256\n",
       "│    │    └─MultiheadAttention: 3-6      [1, 257, 128]             66,048\n",
       "│    │    └─LayerNorm: 3-7               [1, 257, 128]             256\n",
       "│    │    └─Sequential: 3-8              [1, 257, 128]             131,712\n",
       "├─Dropout: 1-6                           [1, 257, 128]             --\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Attention: 2-5                    [1, 257, 128]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 257, 128]             256\n",
       "│    │    └─MultiheadAttention: 3-10     [1, 257, 128]             66,048\n",
       "│    │    └─LayerNorm: 3-11              [1, 257, 128]             256\n",
       "│    │    └─Sequential: 3-12             [1, 257, 128]             131,712\n",
       "├─Dropout: 1-8                           [1, 257, 128]             --\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Attention: 2-6                    [1, 257, 128]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 257, 128]             256\n",
       "│    │    └─MultiheadAttention: 3-14     [1, 257, 128]             66,048\n",
       "│    │    └─LayerNorm: 3-15              [1, 257, 128]             256\n",
       "│    │    └─Sequential: 3-16             [1, 257, 128]             131,712\n",
       "├─Dropout: 1-10                          [1, 257, 128]             --\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Attention: 2-7                    [1, 257, 128]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 257, 128]             256\n",
       "│    │    └─MultiheadAttention: 3-18     [1, 257, 128]             66,048\n",
       "│    │    └─LayerNorm: 3-19              [1, 257, 128]             256\n",
       "│    │    └─Sequential: 3-20             [1, 257, 128]             131,712\n",
       "├─Dropout: 1-12                          [1, 257, 128]             --\n",
       "├─ModuleList: 1-13                       --                        (recursive)\n",
       "│    └─Attention: 2-8                    [1, 257, 128]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 257, 128]             256\n",
       "│    │    └─MultiheadAttention: 3-22     [1, 257, 128]             66,048\n",
       "│    │    └─LayerNorm: 3-23              [1, 257, 128]             256\n",
       "│    │    └─Sequential: 3-24             [1, 257, 128]             131,712\n",
       "├─Sequential: 1-14                       [1, 525]                  --\n",
       "│    └─LayerNorm: 2-9                    [1, 128]                  256\n",
       "│    └─Linear: 2-10                      [1, 525]                  67,725\n",
       "==========================================================================================\n",
       "Total params: 1,366,029\n",
       "Trainable params: 1,366,029\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.94\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 11.32\n",
       "Params size (MB): 3.75\n",
       "Estimated Total Size (MB): 15.67\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionTransformer(hidden_dim = 512, num_heads = 8, num_layers = 6, embedding_dim=128, dropout=0.1).to(device)\n",
    "\n",
    "summary(model, input_size=(1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f682ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 40], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9133c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 55\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = total_loss / len(val_loader.dataset)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch') as tqdm_loader:\n",
    "        for inputs, labels in tqdm_loader:\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            # Update progress bar description with current loss and accuracy\n",
    "            tqdm_loader.set_postfix({'Loss': loss.item(), 'Accuracy': correct_predictions / total_predictions})\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Compute epoch loss and accuracy\n",
    "    epoch_loss = total_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        # Add other relevant information as needed\n",
    "    }\n",
    "    torch.save(checkpoint, f'{checkpoint_dir}/checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    \n",
    "    # Print training progress and validation metrics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Print training completed\n",
    "print('Training completed!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
